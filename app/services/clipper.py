import logging
import time
import os
import requests
import html2text
from pathlib import Path
from bs4 import BeautifulSoup
from typing import Dict, Tuple, Any

from app.services.github import GitHubService
from app.services.notion import NotionService
from app.services.telegram import TelegramService
from app.services.ai import AIService
from app.utils import parse_filename

logger = logging.getLogger(__name__)

class WebClipperHandler:
    """ç½‘é¡µå‰ªè—å¤„ç†å™¨ï¼Œåè°ƒå„ä¸ªæœåŠ¡"""
    
    def __init__(self):
        self.github_service = GitHubService()
        self.notion_service = NotionService()
        self.telegram_service = TelegramService()
        self.ai_service = AIService()
        
    async def process_file(self, file_path: Path, original_url: str = '') -> Dict[str, str]:
        """
        å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            original_url: åŸå§‹URL
            
        Returns:
            dict: å¤„ç†ç»“æœ
        """
        try:
            logger.info("ğŸ”„ å¼€å§‹å¤„ç†æ–°çš„ç½‘é¡µå‰ªè—...")
            
            # 1. ä¸Šä¼ åˆ° GitHub Pages
            filename, github_url = self.github_service.upload_file(file_path)
            logger.info(f"ğŸ“¤ GitHub ä¸Šä¼ æˆåŠŸ: {github_url}")
            
            # 2. è·å–é¡µé¢å†…å®¹è½¬æ¢ä¸º Markdown
            md_content = self.url_to_markdown(github_url)
            
            # 3. è·å–é¡µé¢æ ‡é¢˜
            title = self.extract_title_from_markdown(md_content)
            logger.info(f"ğŸ“‘ é¡µé¢æ ‡é¢˜: {title}")
            
            # å¦‚æœæ²¡æœ‰æä¾›åŸå§‹ URLï¼Œåˆ™ä»æ–‡ä»¶åè§£æ
            if not original_url:
                file_info = parse_filename(filename)
                original_url = file_info['original_url']
            
            # 4. ç”Ÿæˆæ‘˜è¦å’Œæ ‡ç­¾
            summary, tags = self.ai_service.generate_summary_and_tags(md_content)
            logger.info(f"ğŸ“ æ‘˜è¦: {summary[:100]}...")
            logger.info(f"ğŸ·ï¸ æ ‡ç­¾: {', '.join(tags)}")
            
            # 5. ä¿å­˜åˆ° Notion
            notion_url = self.notion_service.save_page({
                'title': title,
                'original_url': original_url,
                'snapshot_url': github_url,
                'summary': summary,
                'tags': tags,
                'created_at': time.time()
            })
            logger.info(f"ğŸ““ Notion ä¿å­˜æˆåŠŸ")
            
            # 6. å‘é€ Telegram é€šçŸ¥
            notification = (
                f"âœ¨ æ–°çš„ç½‘é¡µå‰ªè—\n\n"
                f"ğŸ“‘ {title}\n\n"
                f"ğŸ“ {summary}\n\n"
                f"ğŸ”— åŸå§‹é“¾æ¥ï¼š{original_url}\n"
                f"ğŸ“š å¿«ç…§é“¾æ¥ï¼š{github_url}"
            )
            await self.telegram_service.send_notification(notification)
            
            logger.info("=" * 50)
            logger.info("âœ¨ ç½‘é¡µå‰ªè—å¤„ç†å®Œæˆ!")
            logger.info(f"ğŸ“ åŸå§‹é“¾æ¥: {original_url}")
            logger.info(f"ğŸ”— GitHubé¢„è§ˆ: {github_url}")
            logger.info(f"ğŸ“š Notionç¬”è®°: {notion_url}")
            logger.info("=" * 50)
            
            return {
                "status": "success",
                "github_url": github_url,
                "notion_url": notion_url
            }
            
        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            logger.error("=" * 50)
            
            # å°è¯•å‘é€å¤±è´¥é€šçŸ¥
            try:
                await self.telegram_service.send_notification(error_msg)
            except:
                pass
                
            raise
            
    def url_to_markdown(self, url: str, max_retries: int = 30) -> str:
        """
        å°† URL è½¬æ¢ä¸º Markdown
        
        Args:
            url: é¡µé¢ URL
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            str: Markdown å†…å®¹
        """
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨ r.jina.ai æœåŠ¡
            for attempt in range(max_retries):
                try:
                    md_url = f"https://r.jina.ai/{url}"
                    response = requests.get(md_url, timeout=30)
                    if response.status_code == 200:
                        return response.text
                except Exception as e:
                    logger.debug(f"r.jina.ai è½¬æ¢å¤±è´¥ (å°è¯• {attempt+1}): {str(e)}")
                    
                # ä¼‘çœ åé‡è¯•
                time.sleep(10)
                
            # å¦‚æœ r.jina.ai å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°è§£æ
            logger.info("r.jina.ai è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°è§£æ...")
            return self.extract_content_from_html(url)
            
        except Exception as e:
            logger.error(f"URL è½¬æ¢ä¸º Markdown å¤±è´¥: {str(e)}")
            # å¤±è´¥æ—¶ä»è¿”å›ç½‘é¡µ URL ä½œä¸ºå†…å®¹
            return f"Title: {url} \n\n Content could not be extracted."
    
    def extract_title_from_markdown(self, md_content: str) -> str:
        """
        ä» Markdown å†…å®¹æå–æ ‡é¢˜
        
        Args:
            md_content: Markdown å†…å®¹
            
        Returns:
            str: é¡µé¢æ ‡é¢˜
        """
        # æŸ¥æ‰¾ Title è¡Œ
        lines = md_content.splitlines()
        for line in lines:
            if line.startswith("Title:"):
                return line.replace("Title:", "").strip()
                
        # å¦‚æœæ‰¾ä¸åˆ°ï¼ŒæŸ¥æ‰¾ç¬¬ä¸€ä¸ª Markdown æ ‡é¢˜
        for line in lines:
            if line.startswith("# "):
                return line.replace("#", "").strip()
                
        return "æœªçŸ¥æ ‡é¢˜"
    
    def extract_content_from_html(self, url: str, max_retries: int = 60) -> str:
        """
        ä» HTML é¡µé¢æå–å†…å®¹
        
        Args:
            url: é¡µé¢ URL
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            str: æå–çš„å†…å®¹
        """
        for attempt in range(max_retries):
            try:
                response = requests.get(url, timeout=30)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # è·å–æ ‡é¢˜
                    title = self._extract_title_from_soup(soup) or os.path.basename(url)
                    
                    # æå–æ­£æ–‡å†…å®¹
                    html2markdown = html2text.HTML2Text()
                    html2markdown.ignore_links = True
                    html2markdown.ignore_images = True
                    content = html2markdown.handle(soup.prettify())
                    
                    return f"Title: {title} \n\n {content}"
                    
            except Exception as e:
                logger.debug(f"HTML æå–å¤±è´¥ (å°è¯• {attempt+1}): {str(e)}")
                
            # ä¼‘çœ åé‡è¯•
            time.sleep(5)
            
        return f"Title: {os.path.basename(url)} \n\n Content could not be extracted."
    
    def _extract_title_from_soup(self, soup: BeautifulSoup) -> str:
        """ä» BeautifulSoup å¯¹è±¡æå–æ ‡é¢˜"""
        # ä¼˜å…ˆä½¿ç”¨ title æ ‡ç­¾
        if soup.title:
            return soup.title.string
            
        # å…¶æ¬¡ä½¿ç”¨ h1 æ ‡ç­¾
        if soup.h1:
            return soup.h1.get_text(strip=True)
            
        # æœ€åå°è¯•å…¶ä»–æ ‡é¢˜æ ‡ç­¾
        for tag in ['h2', 'h3', 'h4', 'h5', 'h6']:
            if soup.find(tag):
                return soup.find(tag).get_text(strip=True)
                
        return None